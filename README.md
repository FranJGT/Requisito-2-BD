# Requisito 2: Preprocesamiento y Vectorizaci√≥n del Corpus

## üìã Descripci√≥n
Este requisito procesa un corpus de discursos hist√≥ricos, genera embeddings sem√°nticos y los almacena en MongoDB para b√∫squedas por similitud.

## üöÄ Gu√≠a de Ejecuci√≥n Paso a Paso

### 1Ô∏è‚É£ Prerrequisitos

Aseg√∫rate de tener instalado:
- Python 3.8 o superior
- Docker Desktop
- MongoDB ejecut√°ndose en contenedores (del Requisito 1)

Verifica que MongoDB est√© funcionando:
```bash
docker ps | grep mongo
```

Deber√≠as ver 3 contenedores: mongo1, mongo2, mongo3

### 2Ô∏è‚É£ Preparar el Entorno

#### Clonar o crear el directorio del proyecto:
```bash
mkdir laboratorio2
cd laboratorio2
```

#### Crear entorno virtual (recomendado):
```bash
# En macOS/Linux
python3 -m venv venv
source venv/bin/activate

# En Windows
python -m venv venv
venv\Scripts\activate
```

### 3Ô∏è‚É£ Instalar Dependencias

#### Crear archivo `requirements.txt`:
```bash
cat > requirements.txt << EOF
pymongo==4.6.1
sentence-transformers==2.3.1
numpy==1.24.3
torch>=2.0.0
transformers>=4.36.0
tqdm==4.66.1
EOF
```

#### Instalar las dependencias:
```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Preparar el Corpus de Discursos

#### Descomprimir los archivos de discursos:
```bash
# Si tienes el archivo .rar
unrar x DiscursosOriginales.rar

# O si tienes el archivo .zip
unzip DiscursosOriginales.zip
```

#### Verificar que los archivos est√©n en la carpeta correcta:
```bash
ls DiscursosOriginales/*.txt | head -5
```

Deber√≠as ver archivos como: `91265.txt`, `83107.txt`, etc.

### 5Ô∏è‚É£ Crear el Script de Procesamiento

#### Crear archivo `requisito2_procesamiento.py`:
```bash
cat > requisito2_procesamiento.py << 'EOF'
import os
import hashlib
import json
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, List, Tuple

import numpy as np
from sentence_transformers import SentenceTransformer
from pymongo import MongoClient, WriteConcern, errors
from tqdm import tqdm

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('procesamiento.log'),
        logging.StreamHandler()
    ]
)

class CorpusProcessor:
    """Procesador de corpus de discursos para vectorizaci√≥n e inserci√≥n en MongoDB"""
    
    def __init__(self, corpus_path: str, mongo_uri: str):
        self.corpus_path = Path(corpus_path)
        self.mongo_uri = mongo_uri
        
        # Inicializar modelo de embeddings
        logging.info("Cargando modelo de embeddings...")
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Conectar a MongoDB
        logging.info("Conectando a MongoDB...")
        self.client = MongoClient(mongo_uri)
        self.db = self.client['Politica']
        self.collection = self.db['Discursos']
        
        # Estad√≠sticas
        self.stats = {
            'procesados': 0,
            'errores': 0,
            'duplicados': 0,
            'tiempo_inicio': datetime.now()
        }
    
    def generate_sha256(self, text: str) -> str:
        """Genera hash SHA-256 del texto"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    def generate_embedding(self, text: str) -> List[float]:
        """Genera embedding del texto usando sentence-transformers"""
        try:
            embedding = self.model.encode(text, show_progress_bar=False)
            return embedding.tolist()
        except Exception as e:
            logging.error(f"Error generando embedding: {e}")
            raise
    
    def process_file(self, filepath: Path) -> Dict:
        """Procesa un archivo individual y retorna el documento"""
        try:
            # Leer archivo
            with open(filepath, 'r', encoding='utf-8') as f:
                texto = f.read().strip()
            
            if not texto:
                raise ValueError("Archivo vac√≠o")
            
            # Generar hash y embedding
            doc_id = self.generate_sha256(texto)
            embedding = self.generate_embedding(texto)
            
            # Crear documento
            documento = {
                "_id": doc_id,
                "texto": texto,
                "embedding": embedding,
                "metadata": {
                    "archivo_original": filepath.name,
                    "fecha_procesamiento": datetime.now(),
                    "longitud_texto": len(texto),
                    "primeras_palabras": ' '.join(texto.split()[:10]) + "..."
                }
            }
            
            return documento
            
        except Exception as e:
            logging.error(f"Error procesando {filepath.name}: {str(e)}")
            raise
    
    def process_corpus(self):
        """Procesa todos los archivos del corpus"""
        # Obtener archivos .txt
        txt_files = list(self.corpus_path.glob('*.txt'))
        total_files = len(txt_files)
        
        if total_files == 0:
            logging.error(f"No se encontraron archivos .txt en {self.corpus_path}")
            return
        
        logging.info(f"Encontrados {total_files} archivos para procesar")
        
        # Procesar cada archivo
        with tqdm(total=total_files, desc="Procesando discursos") as pbar:
            for filepath in txt_files:
                try:
                    # Procesar archivo
                    documento = self.process_file(filepath)
                    
                    # Insertar en MongoDB
                    self.collection.insert_one(documento)
                    
                    self.stats['procesados'] += 1
                    pbar.set_postfix({'Procesados': self.stats['procesados']})
                    
                except errors.DuplicateKeyError:
                    logging.warning(f"Documento duplicado: {filepath.name}")
                    self.stats['duplicados'] += 1
                    
                except Exception as e:
                    logging.error(f"Error con {filepath.name}: {str(e)}")
                    self.stats['errores'] += 1
                
                pbar.update(1)
        
        self._print_summary()
    
    def _print_summary(self):
        """Imprime resumen del procesamiento"""
        duracion = (datetime.now() - self.stats['tiempo_inicio']).total_seconds()
        
        print("\n" + "="*60)
        print("RESUMEN DEL PROCESAMIENTO")
        print("="*60)
        print(f"‚úÖ Documentos procesados: {self.stats['procesados']}")
        print(f"‚ö†Ô∏è  Duplicados omitidos: {self.stats['duplicados']}")
        print(f"‚ùå Errores: {self.stats['errores']}")
        print(f"‚è±Ô∏è  Tiempo total: {duracion:.2f} segundos")
        print(f"üìä Documentos en BD: {self.collection.count_documents({})}")
        print("="*60)
    
    def validate_collection(self):
        """Valida la integridad de la colecci√≥n"""
        print("\nüîç VALIDACI√ìN DE LA COLECCI√ìN")
        print("="*60)
        
        # Verificar estructura de un documento
        sample = self.collection.find_one()
        if sample:
            print("‚úÖ Estructura del documento:")
            print(f"   - ID (SHA-256): {sample['_id'][:16]}...")
            print(f"   - Longitud texto: {len(sample['texto'])} caracteres")
            print(f"   - Dimensi√≥n embedding: {len(sample['embedding'])}")
            print(f"   - Metadata: {list(sample.get('metadata', {}).keys())}")
        
        # Verificar consistencia de embeddings
        pipeline = [
            {"$project": {
                "embedding_size": {"$size": "$embedding"}
            }},
            {"$group": {
                "_id": "$embedding_size",
                "count": {"$sum": 1}
            }}
        ]
        
        sizes = list(self.collection.aggregate(pipeline))
        print(f"\n‚úÖ Consistencia de embeddings:")
        for size in sizes:
            print(f"   - Dimensi√≥n {size['_id']}: {size['count']} documentos")
        
        return True

# Script principal
if __name__ == "__main__":
    # Configuraci√≥n
    CORPUS_PATH = "./DiscursosOriginales"
    MONGO_URI = "mongodb://localhost:27017/?directConnection=true"
    
    # Verificar que la carpeta existe
    if not os.path.exists(CORPUS_PATH):
        print(f"‚ùå Error: No se encuentra la carpeta {CORPUS_PATH}")
        print("Aseg√∫rate de haber descomprimido DiscursosOriginales.rar")
        exit(1)
    
    # Crear procesador
    processor = CorpusProcessor(CORPUS_PATH, MONGO_URI)
    
    # Procesar corpus
    processor.process_corpus()
    
    # Validar resultados
    processor.validate_collection()
EOF
```

### 6Ô∏è‚É£ Ejecutar el Procesamiento

#### Ejecutar el script:
```bash
python3 requisito2_procesamiento.py
```

#### Resultado esperado:
```
2025-06-05 19:15:41,292 - INFO - Cargando modelo de embeddings...
2025-06-05 19:15:44,853 - INFO - Conectando a MongoDB...
2025-06-05 19:15:44,861 - INFO - Encontrados 680 archivos para procesar
Procesando discursos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 680/680 [00:13<00:00, 50.79it/s]

============================================================
RESUMEN DEL PROCESAMIENTO
============================================================
‚úÖ Documentos procesados: 679
‚ö†Ô∏è  Duplicados omitidos: 1
‚ùå Errores: 0
‚è±Ô∏è  Tiempo total: 13.40 segundos
üìä Documentos en BD: 679
============================================================
```

### 7Ô∏è‚É£ Verificar los Resultados

#### Conectarse a MongoDB y verificar:
```bash
docker exec -it mongo1 mongosh
```

#### Dentro de mongosh:
```javascript
// Cambiar a la base de datos
use Politica

// Contar documentos
db.Discursos.countDocuments()

// Ver un documento de ejemplo
db.Discursos.findOne()

// Verificar que todos tienen embeddings
db.Discursos.find({"embedding": {"$exists": false}}).count()

// Salir
exit
```

### 8Ô∏è‚É£ Verificaci√≥n con Script Python

#### Crear archivo `verificar_requisito2.py`:
```bash
cat > verificar_requisito2.py << 'EOF'
from pymongo import MongoClient

# Conectar a MongoDB
client = MongoClient('mongodb://localhost:27017/?directConnection=true')
db = client['Politica']
collection = db['Discursos']

# Estad√≠sticas
total = collection.count_documents({})
con_embeddings = collection.count_documents({"embedding": {"$exists": True}})
sin_embeddings = collection.count_documents({"embedding": {"$exists": False}})

# Obtener un documento de muestra
muestra = collection.find_one()

print("üìä VERIFICACI√ìN DEL REQUISITO 2")
print("="*50)
print(f"‚úÖ Total de documentos: {total}")
print(f"‚úÖ Documentos con embeddings: {con_embeddings}")
print(f"‚ùå Documentos sin embeddings: {sin_embeddings}")

if muestra:
    print(f"\nüìÑ Documento de muestra:")
    print(f"   - ID: {muestra['_id'][:16]}...")
    print(f"   - Longitud del texto: {len(muestra['texto'])} caracteres")
    print(f"   - Dimensi√≥n del embedding: {len(muestra['embedding'])}")
    print(f"   - Metadata: {list(muestra.get('metadata', {}).keys())}")

print("\n‚úÖ Requisito 2 completado exitosamente!" if sin_embeddings == 0 else "‚ùå Hay documentos sin procesar")
EOF
```

#### Ejecutar la verificaci√≥n:
```bash
python3 verificar_requisito2.py
```

## üêõ Soluci√≥n de Problemas

### Error: "No se encuentra la carpeta DiscursosOriginales"
```bash
# Verificar contenido del directorio actual
ls -la

# Si los archivos est√°n en otro lugar, mover o actualizar CORPUS_PATH en el script
```

### Error: "Connection refused" al conectar a MongoDB
```bash
# Verificar que los contenedores est√©n ejecut√°ndose
docker ps

# Si no est√°n ejecut√°ndose, iniciarlos
docker-compose up -d
```

### Error: "Module not found"
```bash
# Asegurarse de que el entorno virtual est√© activado
source venv/bin/activate  # macOS/Linux
# o
venv\Scripts\activate  # Windows

# Reinstalar dependencias
pip install -r requirements.txt
```

### Procesamiento muy lento
```bash
# El modelo se descarga la primera vez (puede tomar varios minutos)
# Las siguientes ejecuciones ser√°n m√°s r√°pidas
```

## üìù Logs y Salidas

Los logs se guardan en:
- `procesamiento.log` - Log detallado del procesamiento
- Terminal - Progreso en tiempo real

## ‚úÖ Criterios de √âxito

El requisito se considera completado cuando:
1. ‚úÖ Se procesan al menos 650 documentos (95% del total)
2. ‚úÖ Todos los documentos tienen embeddings de 384 dimensiones
3. ‚úÖ No hay errores cr√≠ticos durante el procesamiento
4. ‚úÖ La validaci√≥n final es exitosa

## üéØ Siguiente Paso

Una vez completado el Requisito 2, puedes proceder con:
- **Requisito 3**: Sistema de b√∫squeda sem√°ntica
- **Requisito 4**: Pruebas de alta disponibilidad

---
